{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acos_templates = [\n",
    "# '''In the ACOS task, you are required to extract the Aspect (a specific detail of the topic), Category (the general domain of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (the descriptive sentiment regarding the aspect). For the provided sentence from the {domain} domain, extract ACOS quadruples in the format: \"[Aspect, Category, Sentiment, Opinion],...\". If any element is missing, replace it with \"NULL\". Sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# '''In the ACOS task, your goal is to identify: Aspect (specific details of a subject), Category (broad classification of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (description of the sentiment towards the aspect). For the input sentence from the {domain}, provide the ACOS elements using this pattern: \"[Aspect, Category, Sentiment, Opinion],...\". Use \"NULL\" for missing elements. Analyze the sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# '''You're tasked with the ACOS extraction. This involves recognizing the Aspect (particular element of interest), Category (overall domain of the aspect), Sentiment (can be Postive, Negative, or Neutral), and the Opinion (how the sentiment is described). Given a sentence from the {domain}, list the ACOS details as: \"[Aspect, Category, Sentiment, Opinion],...\". If a component isn't present, write \"NULL\". Consider this sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# '''For this task, pinpoint the following from the sentence: Aspect (unique feature or detail), Category (major domain pertaining to the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (expressed sentiment about the aspect). Given a sentence from {domain}, structure your ACOS findings as: \"[Aspect, Category, Sentiment, Opinion],...\". In the absence of any detail, fill with \"NULL\". Here's your sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# '''Engage in the ACOS activity. Here, you'll distinguish the Aspect (an individual characteristic), Category (which domain this aspect falls under), Sentiment (either Postive, Negative, or Neutral), and Opinion (the elaboration of the sentiment). With a sentence from the {domain}, represent ACOS in this manner: \"[Aspect, Category, Sentiment, Opinion],...\". \"NULL\" should be used for elements that are not identified. Dive into the sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# '''Embark on the ACOS challenge. Detect the Aspect (a standout feature), Category (the overarching domain of the aspect), Sentiment (classified as Postive, Negative, or Neutral), and Opinion (detailed expression of the sentiment). When presented with a sentence from {domain}, arrange ACOS details like: \"[Aspect, Category, Sentiment, Opinion],...\". Should any piece be absent, slot in \"NULL\". Focus on the sentence: \\n\"{sentence}\"\n",
    "# ''',\n",
    "# # 改变句子位置和任务详细度\n",
    "# '''Sentence from {domain}: \\n\"{sentence}\"\\nFor this analysis, you'll work with ACOS. This involves pinning down the Aspect, which is a specific detail or characteristic. Also, you should identify the Category as the overarching domain. Sentiment can be Postive, Negative, or Neutral, and finally, define the Opinion, which is a descriptive sentiment. Extract and format as: \"[Aspect, Category, Sentiment, Opinion],...\". Missing elements? Use \"NULL\".\n",
    "# ''',\n",
    "# '''Start with this sentence from {domain}: \\n\"{sentence}\"\\nNow, dive into ACOS extraction. Focus on the Aspect (a unique trait or feature) and its Category (its larger domain). Further, detect the Sentiment (the emotional tone) and describe it using the Opinion. Present your findings like: \"[Aspect, Category, Sentiment, Opinion],...\". Fill gaps with \"NULL\".\n",
    "# ''',\n",
    "# '''Here's a sentence from {domain} to analyze: \\n\"{sentence}\"\\nFor ACOS, identify the Aspect, which refers to a unique feature. Classify this feature into a Category or domain. Understand the Sentiment behind it, whether it's Positive, Negative, or Neutral. Also, elaborate on this emotion as an Opinion. Your extraction should be: \"[Aspect, Category, Sentiment, Opinion],...\". Any missing part should be marked as \"NULL\".\n",
    "# ''',\n",
    "# # 更简略的任务定义\n",
    "# '''Sentence: \\n\"{sentence}\"\\nFrom the {domain} domain, extract ACOS details. Use the format: \"[Aspect, Category, Sentiment, Opinion],...\". Replace missing parts with \"NULL\".\n",
    "# ''',\n",
    "# '''Analyze the sentence from {domain}: \\n\"{sentence}\"\\nComplete the ACOS task and format like: \"[Aspect, Category, Sentiment, Opinion],...\". Not all elements may be present, so use \"NULL\" accordingly.\n",
    "# ''',\n",
    "# # 使用问题形式\n",
    "# '''Given the sentence from {domain} below, how would you represent its Aspect, Category, Sentiment, and Opinion? \\n\"{sentence}\"\\nYour answer should look like: \"[Aspect, Category, Sentiment, Opinion],...\". If unsure about any part, write \"NULL\".\n",
    "# ''',\n",
    "# '''What can you infer in terms of Aspect, Category, Sentiment, and Opinion from this {domain} sentence? \\n\"{sentence}\"\\nStructure your insights as: \"[Aspect, Category, Sentiment, Opinion],...\". \"NULL\" is for the indeterminate sections.\n",
    "# ''',\n",
    "# # 更细致的任务定义\n",
    "# '''From the {domain} domain, you're presented with a sentence: \\n\"{sentence}\"\\nEngage in the ACOS task. First, identify the Aspect - this is the standout feature or detail. Then, classify this detail under a Category, which is the broad domain it relates to. Moving on, gauge the Sentiment, capturing the emotion as either Positive, Negative, or Neutral. Finally, express this emotion descriptively, terming it as the Opinion. Represent findings as: \"[Aspect, Category, Sentiment, Opinion],...\". Absences are noted with \"NULL\".\n",
    "# ''',\n",
    "# '''Sentence to decode from {domain}: \\n\"{sentence}\"\\nYour mission is ACOS extraction. Start with the Aspect, a specific facet or characteristic. Next, allocate this facet to its rightful Category, the larger domain. Further, determine the Sentiment behind it - is it Positive, Negative, or Neutral? Detail this sentiment with the Opinion. Your report should be styled as: \"[Aspect, Category, Sentiment, Opinion],...\". If a section evades detection, plug in \"NULL\".\n",
    "# ''',\n",
    "# '''You've received a sentence from the {domain} realm: \\n\"{sentence}\"\\nEmbark on an ACOS analysis. Pinpoint the Aspect, which is a distinct feature. Group this feature under a Category, reflecting its wider domain. After that, assess the Sentiment, encapsulating the emotion either as Positive, Negative, or Neutral. Elaborate on this emotion, designating it as the Opinion. Display results in the pattern: \"[Aspect, Category, Sentiment, Opinion],...\". Incomplete sections should be labeled \"NULL\".\n",
    "# '''\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    'acos': [\n",
    "            '''In the ACOS task, you are required to extract the Aspect (a specific detail of the topic), Category (the general domain of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (the descriptive sentiment regarding the aspect). For the provided sentence from the {domain} domain, extract ACOS quadruples in the format: \"[Aspect, Category, Sentiment, Opinion],...\". If any element is missing, replace it with \"NULL\". Sentence: \\n\"{sentence}\"\n",
    "            ''',\n",
    "            '''In the ACOS task, your goal is to identify: Aspect (specific details of a subject), Category (broad classification of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (description of the sentiment towards the aspect). For the input sentence from the {domain}, provide the ACOS elements using this pattern: \"[Aspect, Category, Sentiment, Opinion],...\". Use \"NULL\" for missing elements. Analyze the sentence: \\n\"{sentence}\"\n",
    "            ''',\n",
    "            '''You're tasked with the ACOS extraction. This involves recognizing the Aspect (particular element of interest), Category (overall domain of the aspect), Sentiment (can be Postive, Negative, or Neutral), and the Opinion (how the sentiment is described). Given a sentence from the {domain}, list the ACOS details as: \"[Aspect, Category, Sentiment, Opinion],...\". If a component isn't present, write \"NULL\". Consider this sentence: \\n\"{sentence}\"\n",
    "            ''',\n",
    "            '''For this task, pinpoint the following from the sentence: Aspect (unique feature or detail), Category (major domain pertaining to the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (expressed sentiment about the aspect). Given a sentence from {domain}, structure your ACOS findings as: \"[Aspect, Category, Sentiment, Opinion],...\". In the absence of any detail, fill with \"NULL\". Here's your sentence: \\n\"{sentence}\"\n",
    "            ''',\n",
    "            '''Engage in the ACOS activity. Here, you'll distinguish the Aspect (an individual characteristic), Category (which domain this aspect falls under), Sentiment (either Postive, Negative, or Neutral), and Opinion (the elaboration of the sentiment). With a sentence from the {domain}, represent ACOS in this manner: \"[Aspect, Category, Sentiment, Opinion],...\". \"NULL\" should be used for elements that are not identified. Dive into the sentence: \\n\"{sentence}\"\n",
    "            '''\n",
    "            ],\n",
    "    'aste':[\n",
    "    '''Given the sentence from the {domain} domain below, please perform the ASTE task. This means identifying the Aspect (specific detail of sentiment), Sentiment (which can be Positive, Negative, or Neutral), and the Opinion (descriptive sentiment for the aspect). Format your findings as: \"[Aspect, Sentiment, Opinion],...\". If unsure about any part, write \"NULL\". \\nSentence: \"{sentence}\"\n",
    "''',\n",
    "\n",
    "    '''Here's a sentence from {domain} for analysis: \\n\"{sentence}\"\\nFor ASTE, pinpoint the Aspect (detail expressing sentiment), the Sentiment (Positive, Negative, or Neutral), and describe the sentiment using the Opinion. Represent your extraction as: \"[Aspect, Sentiment, Opinion],...\". Use \"NULL\" for indeterminate sections.\n",
    "''',\n",
    "\n",
    "    '''Dive into this sentence from {domain}: \\n\"{sentence}\"\\nYour task is ASTE extraction. Recognize the Aspect (a particular element showing sentiment), the Sentiment (is it Positive, Negative, or Neutral?), and further elaborate with the Opinion. Your insights should be structured as: \"[Aspect, Sentiment, Opinion],...\". \"NULL\" is for the elusive elements.\n",
    "'''\n",
    "    ],\n",
    "    'as': [\n",
    "    '''From the {domain} domain, decode this sentence: \\n\"{sentence}\"\\nEngage in the AS task. Identify the Aspect (detail or feature of the topic) and its associated Sentiment (either Positive, Negative, or Neutral). Present findings as: \"[Aspect, Sentiment],...\". Use \"NULL\" if any part is missing.\n",
    "''',\n",
    "\n",
    "    '''Analyze this sentence from {domain}: \\n\"{sentence}\"\\nYour mission, AS extraction, involves recognizing the Aspect (a specific trait or characteristic) and its Sentiment (Positive, Negative, or Neutral). Report as: \"[Aspect, Sentiment],...\". Fill in with \"NULL\" for incomplete sections.\n",
    "''',\n",
    "\n",
    "    '''Given this sentence from {domain}: \\n\"{sentence}\"\\nPerform the AS task. Pin down the Aspect (an element or feature) and determine its Sentiment (Positive, Negative, or Neutral). Structure your outputs as: \"[Aspect, Sentiment],...\". Should an element be absent, insert \"NULL\".\n",
    "'''\n",
    "],  \n",
    "    'ate': [\n",
    "   '''Dive into the provided sentence from the {domain} domain below: \\n\"{sentence}\"\\nYour mission is ATE: Identify and extract only the Aspect Term (specific detail or feature of the topic). Present your findings as: \"[Aspect],...\". If unsure about any term, use \"NULL\".\n",
    "''',\n",
    "\n",
    "    '''Given a sentence from the {domain} domain: \\n\"{sentence}\"\\nEngage in the ATE task. Spot the Aspect Term (a particular trait or characteristic of the topic). Structure your outputs as: \"[Aspect],...\". Should a term be missing, insert \"NULL\".\n",
    "''',\n",
    "\n",
    "    '''Analyze this {domain} sentence: \\n\"{sentence}\"\\nYour task, ATE, involves recognizing and extracting the Aspect Term (a distinct element or feature of the topic). Insights should be noted as: \"[Aspect],...\". Use \"NULL\" for elusive terms.\n",
    "'''\n",
    "    ],\n",
    "    'ote': [\n",
    "    '''From the {domain} domain, decode this sentence: \\n\"{sentence}\"\\nEngage in the OTE task. Your goal is to spot the Opinion Term (words that describe sentiment). Record your findings as: \"[Opinion],...\". Use \"NULL\" if any term is missing.\n",
    "''',\n",
    "\n",
    "    '''Given this sentence from {domain}: \\n\"{sentence}\"\\nPerform the OTE task. Pin down the Opinion Term (an expression or word that indicates sentiment). Structure your outputs as: \"[Opinion],...\". If an expression is absent, denote with \"NULL\".\n",
    "''',\n",
    "\n",
    "    '''Consider the sentence from the {domain} domain: \\n\"{sentence}\"\\nYour mission, OTE, is to recognize and extract the Opinion Term (words reflecting sentiment). Your insights should be styled as: \"[Opinion],...\". Use \"NULL\" for indeterminate terms.\n",
    "'''\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "domains = ['Books', 'Clothing', 'Hotel', 'Laptop', 'Restaurant']\n",
    "path_prefix = \"../data/raw/MEMD_ABSA_Dataset\"\n",
    "data_names = ['Dev.json', 'Train.json', 'Test.json']\n",
    "\n",
    "# 可以选择要同时进行哪些任务\n",
    "tasks = ['acos', 'aste', 'as', 'ate', 'ote']\n",
    "\n",
    "all_data, test_data = [], []\n",
    "\n",
    "# 遍历所有域和数据文件\n",
    "for domain in domains:\n",
    "    for data_name in data_names:\n",
    "        # 读取数据文件\n",
    "        path = f\"{path_prefix}/{domain}/{data_name}\"\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            for entry in data:\n",
    "                # 对每条数据随机选择任务\n",
    "                task = random.choice(tasks)\n",
    "                # 考虑 Aspect 和 Opinion 可能为空\n",
    "                for quad in entry['quadruples']:\n",
    "                    if 'NULL' in quad['aspect']['term']:\n",
    "                        task = 'acos'\n",
    "                    elif 'NULL' in quad['opinion']['term'] and task == 'ote':\n",
    "                        task = random.choice(['acos', 'aste', 'as', 'ate'])\n",
    "                    break\n",
    "\n",
    "                sentence = entry['raw_words']\n",
    "                template = random.choice(templates[task])  # 从模板列表中随机选择一个\n",
    "                prompt = template.format(domain=domain, sentence=sentence)\n",
    "                \n",
    "                # 提取quadruples字段并转换为指定的格式\n",
    "                quadruples = entry.get('quadruples', [])\n",
    "                outputs = []\n",
    "                for quad in quadruples:\n",
    "                    aspect = \" \".join(quad['aspect']['term']) if quad['aspect'] else \"NULL\"\n",
    "                    category = quad['category']\n",
    "                    sentiment = quad['sentiment']\n",
    "                    opinion = \" \".join(quad['opinion']['term']) if quad['opinion'] else \"NULL\"\n",
    "\n",
    "                    # 记得去重\n",
    "                    if task == 'acos' and f\"[{aspect}, {category}, {sentiment}, {opinion}]\" not in outputs:\n",
    "                        outputs.append(f\"[{aspect}, {category}, {sentiment}, {opinion}]\")\n",
    "                    elif task == 'aste' and f\"[{aspect}, {sentiment}, {opinion}]\" not in outputs:\n",
    "                        outputs.append(f\"[{aspect}, {sentiment}, {opinion}]\")\n",
    "                    elif task == 'as' and f\"[{aspect}, {sentiment}]\" not in outputs:\n",
    "                        outputs.append(f\"[{aspect}, {sentiment}]\")\n",
    "                    elif task == 'ate' and f\"[{aspect}]\" not in outputs:\n",
    "                        outputs.append(f\"[{aspect}]\")\n",
    "                    elif task == 'ote' and f\"[{opinion}]\" not in outputs:\n",
    "                        outputs.append(f\"[{opinion}]\")\n",
    "\n",
    "                output_str = \",\".join(outputs)\n",
    "\n",
    "                new_entry = {\n",
    "                    \"id\": entry.get('id', len(all_data) + 1),  # 使用已有的id或生成一个新的id\n",
    "                    \"domain\": domain,\n",
    "                    \"instruction\": prompt,\n",
    "                    \"input\": \"\", # Dont use the input field\n",
    "                    \"output\": output_str,\n",
    "                    \"task\": task\n",
    "                }\n",
    "\n",
    "                if 'Test' in data_name or 'test' in data_name:\n",
    "                    test_data.append(new_entry)\n",
    "\n",
    "                all_data.append(new_entry)\n",
    "\n",
    "# 将所有数据保存到一个新的JSON文件\n",
    "with open(\"../data/inst/train.json\", 'w') as outfile:\n",
    "    json.dump(all_data, outfile, indent=4)\n",
    "\n",
    "# 将所有数据保存到一个新的JSON文件\n",
    "with open(\"../data/inst/test.json\", 'w') as outfile:\n",
    "    json.dump(all_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 06:05:22,140] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "09/01/2023 06:05:30 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, 16-bits training: True\n",
      "09/01/2023 06:05:30 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=8e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../checkpoints/sa_8_31/runs/Sep01_06-05-30_platform-e204e633-ee5c-4eb1-b24e-3fd17b6eade0,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=../checkpoints/sa_8_31,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../checkpoints/sa_8_31,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=3825,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "09/01/2023 06:05:30 - INFO - llmtuner.dsets.loader - Loading dataset inst/train.json...\n",
      "09/01/2023 06:05:30 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-82981fdc0c44e55f\n",
      "Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 12633.45it/s]\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1318.96it/s]\n",
      "Generating train split\n",
      "Generating train split: 18104 examples [00:00, 28110.67 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "[INFO|tokenization_utils_base.py:1808] 2023-09-01 06:05:32,341 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1808] 2023-09-01 06:05:32,341 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1808] 2023-09-01 06:05:32,341 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1808] 2023-09-01 06:05:32,341 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:667] 2023-09-01 06:05:32,359 >> loading configuration file /hy-tmp/llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-09-01 06:05:32,360 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/hy-tmp/llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2513] 2023-09-01 06:05:32,431 >> loading weights file /hy-tmp/llama-2-7b-hf/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1154] 2023-09-01 06:05:32,434 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 06:05:32,434 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.29.1\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:28<00:00, 14.09s/it]\n",
      "[INFO|modeling_utils.py:3185] 2023-09-01 06:06:02,243 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3193] 2023-09-01 06:06:02,243 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /hy-tmp/llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2023-09-01 06:06:02,249 >> loading configuration file /hy-tmp/llama-2-7b-hf/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-09-01 06:06:02,249 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.29.1\"\n",
      "}\n",
      "\n",
      "09/01/2023 06:06:02 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA\n",
      "09/01/2023 06:06:32 - INFO - llmtuner.tuner.core.loader - trainable params: 33554432 || all params: 6771970048 || trainable%: 0.4955\n",
      "09/01/2023 06:06:32 - INFO - llmtuner.extras.template - Add pad token: <unk>\n",
      "[INFO|tokenization_utils_base.py:908] 2023-09-01 06:06:32,527 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Filter:   0%|                                  | 0/18104 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ee457652c203d86d.arrow\n",
      "Filter: 100%|███████████████████| 18104/18104 [00:00<00:00, 60797.14 examples/s]\n",
      "Running tokenizer on dataset:   0%|            | 0/18104 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-71ab0b75a6f94b37.arrow\n",
      "Running tokenizer on dataset: 100%|█| 18104/18104 [00:50<00:00, 357.74 examples/\n",
      "input_ids:\n",
      "[1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 29892, 3390, 1319, 322, 15993, 20255, 29889, 29849, 1234, 408, 1371, 3730, 408, 1950, 29892, 1550, 1641, 9109, 29889, 29871, 3575, 6089, 881, 451, 3160, 738, 10311, 1319, 29892, 443, 621, 936, 29892, 11021, 391, 29892, 7916, 391, 29892, 304, 27375, 29892, 18215, 29892, 470, 27302, 2793, 29889, 3529, 9801, 393, 596, 20890, 526, 5374, 635, 443, 5365, 1463, 322, 6374, 297, 5469, 29889, 13, 3644, 263, 1139, 947, 451, 1207, 738, 4060, 29892, 470, 338, 451, 2114, 1474, 16165, 261, 296, 29892, 5649, 2020, 2012, 310, 22862, 1554, 451, 1959, 29889, 960, 366, 1016, 29915, 29873, 1073, 278, 1234, 304, 263, 1139, 29892, 3113, 1016, 29915, 29873, 6232, 2089, 2472, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 29954, 5428, 263, 10541, 515, 278, 13730, 5354, 29901, 29871, 13, 29908, 2525, 7524, 1919, 372, 471, 302, 29915, 29873, 701, 304, 610, 411, 902, 4940, 1736, 869, 29908, 13, 8100, 482, 297, 278, 319, 4330, 3414, 29889, 1706, 327, 278, 1094, 1103, 11814, 313, 29874, 3153, 22917, 470, 17443, 310, 278, 11261, 467, 3767, 12425, 596, 14391, 408, 29901, 14704, 2887, 1103, 1402, 856, 1642, 10575, 263, 1840, 367, 4567, 29892, 4635, 376, 10074, 1642, 13, 518, 29914, 25580, 29962, 29871, 518, 10074, 29962, 2]\n",
      "inputs:\n",
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Given a sentence from the Books domain: \n",
      "\"Unfortunately , it was n't up to par with her past works .\"\n",
      "Engage in the ATE task. Spot the Aspect Term (a particular trait or characteristic of the topic). Structure your outputs as: \"[Aspect],...\". Should a term be missing, insert \"NULL\".\n",
      " [/INST]  [NULL]</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 518, 10074, 29962, 2]\n",
      "labels:\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> [NULL]</s>\n",
      "Caching indices mapping at /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-86f52cc4736d3527.arrow\n",
      "Caching indices mapping at /root/.cache/huggingface/datasets/json/default-82981fdc0c44e55f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2bf2db8001191b6c.arrow\n",
      "[INFO|trainer.py:622] 2023-09-01 06:07:38,458 >> Using cuda_amp half precision backend\n",
      "[INFO|trainer.py:1779] 2023-09-01 06:07:38,528 >> ***** Running training *****\n",
      "[INFO|trainer.py:1780] 2023-09-01 06:07:38,528 >>   Num examples = 16,293\n",
      "[INFO|trainer.py:1781] 2023-09-01 06:07:38,528 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:1782] 2023-09-01 06:07:38,528 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1783] 2023-09-01 06:07:38,528 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1784] 2023-09-01 06:07:38,528 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1785] 2023-09-01 06:07:38,528 >>   Total optimization steps = 1,016\n",
      "[INFO|trainer.py:1786] 2023-09-01 06:07:38,530 >>   Number of trainable parameters = 33,554,432\n",
      "09/01/2023 06:07:38 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.\n",
      "{'loss': 1.0929, 'learning_rate': 7.952289157766113e-05, 'epoch': 0.2}          \n",
      "  5%|█▉                                     | 50/1016 [08:06<2:33:20,  9.52s/it][INFO|trainer.py:3165] 2023-09-01 06:15:45,250 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3167] 2023-09-01 06:15:45,251 >>   Num examples = 1811\n",
      "[INFO|trainer.py:3170] 2023-09-01 06:15:45,251 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:44,  2.51it/s]\u001B[A\n",
      "  3%|█▏                                         | 3/114 [00:01<01:02,  1.77it/s]\u001B[A\n",
      "  4%|█▌                                         | 4/114 [00:02<01:04,  1.71it/s]\u001B[A\n",
      "  4%|█▉                                         | 5/114 [00:02<01:07,  1.61it/s]\u001B[A\n",
      "  5%|██▎                                        | 6/114 [00:03<01:14,  1.45it/s]\u001B[A\n",
      "  6%|██▋                                        | 7/114 [00:04<01:13,  1.45it/s]\u001B[A\n",
      "  7%|███                                        | 8/114 [00:05<01:16,  1.39it/s]\u001B[A\n",
      "  8%|███▍                                       | 9/114 [00:05<01:15,  1.38it/s]\u001B[A\n",
      "  9%|███▋                                      | 10/114 [00:06<01:16,  1.36it/s]\u001B[A\n",
      " 10%|████                                      | 11/114 [00:07<01:14,  1.38it/s]\u001B[A\n",
      " 11%|████▍                                     | 12/114 [00:08<01:14,  1.37it/s]\u001B[A\n",
      " 11%|████▊                                     | 13/114 [00:08<01:14,  1.35it/s]\u001B[A\n",
      " 12%|█████▏                                    | 14/114 [00:09<01:12,  1.37it/s]\u001B[A\n",
      " 13%|█████▌                                    | 15/114 [00:10<01:11,  1.39it/s]\u001B[A\n",
      " 14%|█████▉                                    | 16/114 [00:11<01:10,  1.38it/s]\u001B[A\n",
      " 15%|██████▎                                   | 17/114 [00:11<01:14,  1.30it/s]\u001B[A\n",
      " 16%|██████▋                                   | 18/114 [00:12<01:14,  1.28it/s]\u001B[A\n",
      " 17%|███████                                   | 19/114 [00:13<01:14,  1.28it/s]\u001B[A\n",
      " 18%|███████▎                                  | 20/114 [00:14<01:11,  1.32it/s]\u001B[A\n",
      " 18%|███████▋                                  | 21/114 [00:14<01:06,  1.39it/s]\u001B[A\n",
      " 19%|████████                                  | 22/114 [00:15<01:10,  1.31it/s]\u001B[A\n",
      " 20%|████████▍                                 | 23/114 [00:16<01:05,  1.40it/s]\u001B[A\n",
      " 21%|████████▊                                 | 24/114 [00:16<01:01,  1.46it/s]\u001B[A\n",
      " 22%|█████████▏                                | 25/114 [00:17<01:00,  1.46it/s]\u001B[A\n",
      " 23%|█████████▌                                | 26/114 [00:18<01:00,  1.44it/s]\u001B[A\n",
      " 24%|█████████▉                                | 27/114 [00:19<01:02,  1.39it/s]\u001B[A\n",
      " 25%|██████████▎                               | 28/114 [00:19<01:06,  1.29it/s]\u001B[A\n",
      " 25%|██████████▋                               | 29/114 [00:20<01:04,  1.32it/s]\u001B[A\n",
      " 26%|███████████                               | 30/114 [00:21<01:04,  1.31it/s]\u001B[A\n",
      " 27%|███████████▍                              | 31/114 [00:22<00:59,  1.40it/s]\u001B[A\n",
      " 28%|███████████▊                              | 32/114 [00:22<01:00,  1.35it/s]\u001B[A\n",
      " 29%|████████████▏                             | 33/114 [00:23<00:59,  1.37it/s]\u001B[A\n",
      " 30%|████████████▌                             | 34/114 [00:24<00:55,  1.43it/s]\u001B[A\n",
      " 31%|████████████▉                             | 35/114 [00:24<00:56,  1.39it/s]\u001B[A\n",
      " 32%|█████████████▎                            | 36/114 [00:25<00:53,  1.46it/s]\u001B[A\n",
      " 32%|█████████████▋                            | 37/114 [00:26<00:55,  1.39it/s]\u001B[A\n",
      " 33%|██████████████                            | 38/114 [00:27<00:54,  1.40it/s]\u001B[A\n",
      " 34%|██████████████▎                           | 39/114 [00:27<00:55,  1.34it/s]\u001B[A\n",
      " 35%|██████████████▋                           | 40/114 [00:28<00:55,  1.32it/s]\u001B[A\n",
      " 36%|███████████████                           | 41/114 [00:29<00:51,  1.41it/s]\u001B[A\n",
      " 37%|███████████████▍                          | 42/114 [00:30<00:52,  1.36it/s]\u001B[A\n",
      " 38%|███████████████▊                          | 43/114 [00:30<00:51,  1.38it/s]\u001B[A\n",
      " 39%|████████████████▏                         | 44/114 [00:31<00:52,  1.34it/s]\u001B[A\n",
      " 39%|████████████████▌                         | 45/114 [00:32<00:51,  1.35it/s]\u001B[A\n",
      " 40%|████████████████▉                         | 46/114 [00:33<00:49,  1.39it/s]\u001B[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:33<00:48,  1.39it/s]\u001B[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:34<00:46,  1.41it/s]\u001B[A\n",
      " 43%|██████████████████                        | 49/114 [00:35<00:45,  1.42it/s]\u001B[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:35<00:46,  1.36it/s]\u001B[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:36<00:47,  1.33it/s]\u001B[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:37<00:47,  1.30it/s]\u001B[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:38<00:47,  1.29it/s]\u001B[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:39<00:47,  1.28it/s]\u001B[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:39<00:43,  1.35it/s]\u001B[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:40<00:43,  1.34it/s]\u001B[A\n",
      " 50%|█████████████████████                     | 57/114 [00:41<00:42,  1.35it/s]\u001B[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:41<00:42,  1.33it/s]\u001B[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:42<00:40,  1.36it/s]\u001B[A\n",
      " 53%|██████████████████████                    | 60/114 [00:43<00:39,  1.38it/s]\u001B[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:44<00:41,  1.29it/s]\u001B[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:45<00:39,  1.33it/s]\u001B[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:45<00:37,  1.35it/s]\u001B[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:46<00:36,  1.36it/s]\u001B[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:47<00:35,  1.40it/s]\u001B[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:47<00:35,  1.35it/s]\u001B[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:48<00:34,  1.37it/s]\u001B[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:49<00:33,  1.37it/s]\u001B[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:50<00:32,  1.40it/s]\u001B[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:50<00:31,  1.41it/s]\u001B[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:51<00:30,  1.40it/s]\u001B[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:52<00:29,  1.43it/s]\u001B[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:52<00:29,  1.37it/s]\u001B[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:53<00:29,  1.37it/s]\u001B[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:54<00:28,  1.35it/s]\u001B[A\n",
      " 67%|████████████████████████████              | 76/114 [00:55<00:27,  1.36it/s]\u001B[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:55<00:26,  1.39it/s]\u001B[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:56<00:25,  1.39it/s]\u001B[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:57<00:25,  1.39it/s]\u001B[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:57<00:24,  1.37it/s]\u001B[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:58<00:24,  1.33it/s]\u001B[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [00:59<00:24,  1.30it/s]\u001B[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [01:00<00:24,  1.29it/s]\u001B[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [01:01<00:22,  1.33it/s]\u001B[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [01:01<00:22,  1.30it/s]\u001B[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [01:02<00:20,  1.37it/s]\u001B[A\n",
      " 76%|████████████████████████████████          | 87/114 [01:03<00:21,  1.24it/s]\u001B[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [01:04<00:21,  1.21it/s]\u001B[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [01:05<00:20,  1.22it/s]\u001B[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [01:05<00:18,  1.28it/s]\u001B[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [01:06<00:18,  1.27it/s]\u001B[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [01:07<00:17,  1.26it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [01:08<00:16,  1.26it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [01:09<00:15,  1.30it/s]\u001B[A\n",
      " 83%|███████████████████████████████████       | 95/114 [01:09<00:14,  1.34it/s]\u001B[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [01:10<00:13,  1.31it/s]\u001B[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [01:11<00:12,  1.35it/s]\u001B[A\n",
      " 86%|████████████████████████████████████      | 98/114 [01:11<00:11,  1.37it/s]\u001B[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [01:12<00:10,  1.37it/s]\u001B[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [01:13<00:09,  1.46it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [01:13<00:09,  1.39it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [01:14<00:08,  1.40it/s]\u001B[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [01:15<00:08,  1.30it/s]\u001B[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [01:16<00:07,  1.29it/s]\u001B[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [01:17<00:07,  1.23it/s]\u001B[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [01:18<00:06,  1.28it/s]\u001B[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:18<00:05,  1.32it/s]\u001B[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:19<00:04,  1.30it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:20<00:03,  1.34it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:20<00:02,  1.36it/s]\u001B[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:21<00:02,  1.27it/s]\u001B[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:22<00:01,  1.23it/s]\u001B[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:23<00:00,  1.07it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 0.442088782787323, 'eval_runtime': 85.2742, 'eval_samples_per_second': 21.237, 'eval_steps_per_second': 1.337, 'epoch': 0.2}\n",
      "  5%|█▉                                     | 50/1016 [09:31<2:33:20,  9.52s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:24<00:00,  1.12it/s]\u001B[A\n",
      "{'loss': 0.3857, 'learning_rate': 7.810294793297783e-05, 'epoch': 0.39}         \u001B[A\n",
      " 10%|███▋                                  | 100/1016 [17:29<2:18:38,  9.08s/it][INFO|trainer.py:3165] 2023-09-01 06:25:08,050 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3167] 2023-09-01 06:25:08,050 >>   Num examples = 1811\n",
      "[INFO|trainer.py:3170] 2023-09-01 06:25:08,050 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:39,  2.85it/s]\u001B[A\n",
      "  3%|█▏                                         | 3/114 [00:01<01:04,  1.73it/s]\u001B[A\n",
      "  4%|█▌                                         | 4/114 [00:02<01:12,  1.51it/s]\u001B[A\n",
      "  4%|█▉                                         | 5/114 [00:03<01:13,  1.48it/s]\u001B[A\n",
      "  5%|██▎                                        | 6/114 [00:03<01:13,  1.46it/s]\u001B[A\n",
      "  6%|██▋                                        | 7/114 [00:04<01:17,  1.39it/s]\u001B[A\n",
      "  7%|███                                        | 8/114 [00:05<01:18,  1.34it/s]\u001B[A\n",
      "  8%|███▍                                       | 9/114 [00:06<01:20,  1.31it/s]\u001B[A\n",
      "  9%|███▋                                      | 10/114 [00:06<01:17,  1.34it/s]\u001B[A\n",
      " 10%|████                                      | 11/114 [00:07<01:15,  1.37it/s]\u001B[A\n",
      " 11%|████▍                                     | 12/114 [00:08<01:13,  1.38it/s]\u001B[A\n",
      " 11%|████▊                                     | 13/114 [00:09<01:12,  1.40it/s]\u001B[A\n",
      " 12%|█████▏                                    | 14/114 [00:09<01:17,  1.30it/s]\u001B[A\n",
      " 13%|█████▌                                    | 15/114 [00:10<01:14,  1.33it/s]\u001B[A\n",
      " 14%|█████▉                                    | 16/114 [00:11<01:14,  1.31it/s]\u001B[A\n",
      " 15%|██████▎                                   | 17/114 [00:12<01:15,  1.29it/s]\u001B[A\n",
      " 16%|██████▋                                   | 18/114 [00:13<01:15,  1.28it/s]\u001B[A\n",
      " 17%|███████                                   | 19/114 [00:13<01:14,  1.27it/s]\u001B[A\n",
      " 18%|███████▎                                  | 20/114 [00:14<01:08,  1.37it/s]\u001B[A\n",
      " 18%|███████▋                                  | 21/114 [00:15<01:10,  1.33it/s]\u001B[A\n",
      " 19%|████████                                  | 22/114 [00:15<01:07,  1.36it/s]\u001B[A\n",
      " 20%|████████▍                                 | 23/114 [00:16<01:05,  1.38it/s]\u001B[A\n",
      " 21%|████████▊                                 | 24/114 [00:17<01:04,  1.39it/s]\u001B[A\n",
      " 22%|█████████▏                                | 25/114 [00:17<01:01,  1.46it/s]\u001B[A\n",
      " 23%|█████████▌                                | 26/114 [00:18<01:03,  1.39it/s]\u001B[A\n",
      " 24%|█████████▉                                | 27/114 [00:19<01:04,  1.35it/s]\u001B[A\n",
      " 25%|██████████▎                               | 28/114 [00:20<01:05,  1.32it/s]\u001B[A\n",
      " 25%|██████████▋                               | 29/114 [00:21<01:02,  1.35it/s]\u001B[A\n",
      " 26%|███████████                               | 30/114 [00:21<01:01,  1.37it/s]\u001B[A\n",
      " 27%|███████████▍                              | 31/114 [00:22<01:02,  1.33it/s]\u001B[A\n",
      " 28%|███████████▊                              | 32/114 [00:23<01:00,  1.36it/s]\u001B[A\n",
      " 29%|████████████▏                             | 33/114 [00:24<01:01,  1.33it/s]\u001B[A\n",
      " 30%|████████████▌                             | 34/114 [00:24<00:59,  1.35it/s]\u001B[A\n",
      " 31%|████████████▉                             | 35/114 [00:25<00:55,  1.43it/s]\u001B[A\n",
      " 32%|█████████████▎                            | 36/114 [00:26<00:56,  1.38it/s]\u001B[A\n",
      " 32%|█████████████▋                            | 37/114 [00:26<00:53,  1.43it/s]\u001B[A\n",
      " 33%|██████████████                            | 38/114 [00:27<00:54,  1.39it/s]\u001B[A\n",
      " 34%|██████████████▎                           | 39/114 [00:28<00:55,  1.34it/s]\u001B[A\n",
      " 35%|██████████████▋                           | 40/114 [00:28<00:52,  1.41it/s]\u001B[A\n",
      " 36%|███████████████                           | 41/114 [00:29<00:51,  1.42it/s]\u001B[A\n",
      " 37%|███████████████▍                          | 42/114 [00:30<00:50,  1.44it/s]\u001B[A\n",
      " 38%|███████████████▊                          | 43/114 [00:31<00:51,  1.38it/s]\u001B[A\n",
      " 39%|████████████████▏                         | 44/114 [00:31<00:50,  1.39it/s]\u001B[A\n",
      " 39%|████████████████▌                         | 45/114 [00:32<00:51,  1.35it/s]\u001B[A\n",
      " 40%|████████████████▉                         | 46/114 [00:33<00:49,  1.37it/s]\u001B[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:33<00:46,  1.43it/s]\u001B[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:34<00:47,  1.39it/s]\u001B[A\n",
      " 43%|██████████████████                        | 49/114 [00:35<00:46,  1.40it/s]\u001B[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:36<00:47,  1.35it/s]\u001B[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:37<00:47,  1.32it/s]\u001B[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:37<00:47,  1.30it/s]\u001B[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:38<00:47,  1.28it/s]\u001B[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:39<00:47,  1.27it/s]\u001B[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:40<00:43,  1.36it/s]\u001B[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:40<00:43,  1.33it/s]\u001B[A\n",
      " 50%|█████████████████████                     | 57/114 [00:41<00:42,  1.35it/s]\u001B[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:42<00:42,  1.33it/s]\u001B[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:43<00:41,  1.34it/s]\u001B[A\n",
      " 53%|██████████████████████                    | 60/114 [00:43<00:39,  1.37it/s]\u001B[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:44<00:40,  1.29it/s]\u001B[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:45<00:39,  1.33it/s]\u001B[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:46<00:39,  1.31it/s]\u001B[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:46<00:37,  1.33it/s]\u001B[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:47<00:37,  1.32it/s]\u001B[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:48<00:35,  1.34it/s]\u001B[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:49<00:34,  1.37it/s]\u001B[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:49<00:33,  1.38it/s]\u001B[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:50<00:32,  1.40it/s]\u001B[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:51<00:32,  1.35it/s]\u001B[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:51<00:31,  1.38it/s]\u001B[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:52<00:30,  1.37it/s]\u001B[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:53<00:30,  1.35it/s]\u001B[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:54<00:29,  1.37it/s]\u001B[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:54<00:29,  1.34it/s]\u001B[A\n",
      " 67%|████████████████████████████              | 76/114 [00:55<00:27,  1.40it/s]\u001B[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:56<00:25,  1.43it/s]\u001B[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:56<00:25,  1.43it/s]\u001B[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:57<00:24,  1.43it/s]\u001B[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:58<00:24,  1.37it/s]\u001B[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:59<00:24,  1.33it/s]\u001B[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [01:00<00:24,  1.31it/s]\u001B[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [01:00<00:23,  1.32it/s]\u001B[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [01:01<00:22,  1.36it/s]\u001B[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [01:02<00:21,  1.34it/s]\u001B[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [01:03<00:21,  1.31it/s]\u001B[A\n",
      " 76%|████████████████████████████████          | 87/114 [01:03<00:21,  1.24it/s]\u001B[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [01:04<00:21,  1.20it/s]\u001B[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [01:05<00:19,  1.26it/s]\u001B[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [01:06<00:19,  1.26it/s]\u001B[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [01:06<00:17,  1.31it/s]\u001B[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [01:07<00:17,  1.24it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [01:08<00:16,  1.29it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [01:09<00:15,  1.33it/s]\u001B[A\n",
      " 83%|███████████████████████████████████       | 95/114 [01:10<00:14,  1.31it/s]\u001B[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [01:10<00:13,  1.34it/s]\u001B[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [01:11<00:11,  1.42it/s]\u001B[A\n",
      " 86%|████████████████████████████████████      | 98/114 [01:12<00:11,  1.37it/s]\u001B[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [01:12<00:10,  1.38it/s]\u001B[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [01:13<00:10,  1.40it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [01:14<00:09,  1.40it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [01:15<00:08,  1.42it/s]\u001B[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [01:15<00:08,  1.31it/s]\u001B[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [01:16<00:07,  1.29it/s]\u001B[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [01:17<00:07,  1.23it/s]\u001B[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [01:18<00:05,  1.33it/s]\u001B[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:18<00:05,  1.36it/s]\u001B[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:19<00:04,  1.38it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:20<00:03,  1.39it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:20<00:02,  1.45it/s]\u001B[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:21<00:02,  1.29it/s]\u001B[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:22<00:01,  1.27it/s]\u001B[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:24<00:00,  1.06it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 0.323114275932312, 'eval_runtime': 85.3744, 'eval_samples_per_second': 21.212, 'eval_steps_per_second': 1.335, 'epoch': 0.39}\n",
      " 10%|███▋                                  | 100/1016 [18:54<2:18:38,  9.08s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:24<00:00,  1.12it/s]\u001B[A\n",
      "                                                                                \u001B[A09/01/2023 06:26:33 - INFO - llmtuner.tuner.core.trainer - Saving model checkpoint to ../checkpoints/sa_8_31/checkpoint-100\n",
      "{'loss': 0.3094, 'learning_rate': 7.577404241955638e-05, 'epoch': 0.59}         \n",
      " 15%|█████▌                                | 150/1016 [27:06<2:21:50,  9.83s/it][INFO|trainer.py:3165] 2023-09-01 06:34:45,347 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3167] 2023-09-01 06:34:45,347 >>   Num examples = 1811\n",
      "[INFO|trainer.py:3170] 2023-09-01 06:34:45,347 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:44,  2.50it/s]\u001B[A\n",
      "  3%|█▏                                         | 3/114 [00:01<00:58,  1.90it/s]\u001B[A\n",
      "  4%|█▌                                         | 4/114 [00:02<01:09,  1.59it/s]\u001B[A\n",
      "  4%|█▉                                         | 5/114 [00:03<01:11,  1.53it/s]\u001B[A\n",
      "  5%|██▎                                        | 6/114 [00:03<01:15,  1.43it/s]\u001B[A\n",
      "  6%|██▋                                        | 7/114 [00:04<01:16,  1.41it/s]\u001B[A\n",
      "  7%|███                                        | 8/114 [00:05<01:15,  1.41it/s]\u001B[A\n",
      "  8%|███▍                                       | 9/114 [00:06<01:19,  1.32it/s]\u001B[A\n",
      "  9%|███▋                                      | 10/114 [00:06<01:16,  1.35it/s]\u001B[A\n",
      " 10%|████                                      | 11/114 [00:07<01:14,  1.37it/s]\u001B[A\n",
      " 11%|████▍                                     | 12/114 [00:08<01:13,  1.39it/s]\u001B[A\n",
      " 11%|████▊                                     | 13/114 [00:08<01:12,  1.39it/s]\u001B[A\n",
      " 12%|█████▏                                    | 14/114 [00:09<01:16,  1.30it/s]\u001B[A\n",
      " 13%|█████▌                                    | 15/114 [00:10<01:13,  1.34it/s]\u001B[A\n",
      " 14%|█████▉                                    | 16/114 [00:11<01:17,  1.26it/s]\u001B[A\n",
      " 15%|██████▎                                   | 17/114 [00:12<01:17,  1.26it/s]\u001B[A\n",
      " 16%|██████▋                                   | 18/114 [00:13<01:16,  1.25it/s]\u001B[A\n",
      " 17%|███████                                   | 19/114 [00:13<01:15,  1.25it/s]\u001B[A\n",
      " 18%|███████▎                                  | 20/114 [00:14<01:12,  1.30it/s]\u001B[A\n",
      " 18%|███████▋                                  | 21/114 [00:15<01:12,  1.29it/s]\u001B[A\n",
      " 19%|████████                                  | 22/114 [00:16<01:09,  1.32it/s]\u001B[A\n",
      " 20%|████████▍                                 | 23/114 [00:16<01:09,  1.30it/s]\u001B[A\n",
      " 21%|████████▊                                 | 24/114 [00:17<01:07,  1.34it/s]\u001B[A\n",
      " 22%|█████████▏                                | 25/114 [00:18<01:05,  1.36it/s]\u001B[A\n",
      " 23%|█████████▌                                | 26/114 [00:18<01:06,  1.33it/s]\u001B[A\n",
      " 24%|█████████▉                                | 27/114 [00:19<01:04,  1.36it/s]\u001B[A\n",
      " 25%|██████████▎                               | 28/114 [00:20<01:05,  1.32it/s]\u001B[A\n",
      " 25%|██████████▋                               | 29/114 [00:21<01:02,  1.35it/s]\u001B[A\n",
      " 26%|███████████                               | 30/114 [00:22<01:03,  1.32it/s]\u001B[A\n",
      " 27%|███████████▍                              | 31/114 [00:22<01:01,  1.35it/s]\u001B[A\n",
      " 28%|███████████▊                              | 32/114 [00:23<01:02,  1.32it/s]\u001B[A\n",
      " 29%|████████████▏                             | 33/114 [00:24<01:00,  1.35it/s]\u001B[A\n",
      " 30%|████████████▌                             | 34/114 [00:24<00:58,  1.37it/s]\u001B[A\n",
      " 31%|████████████▉                             | 35/114 [00:25<00:57,  1.38it/s]\u001B[A\n",
      " 32%|█████████████▎                            | 36/114 [00:26<00:55,  1.41it/s]\u001B[A\n",
      " 32%|█████████████▋                            | 37/114 [00:27<00:56,  1.35it/s]\u001B[A\n",
      " 33%|██████████████                            | 38/114 [00:27<00:57,  1.32it/s]\u001B[A\n",
      " 34%|██████████████▎                           | 39/114 [00:28<00:55,  1.35it/s]\u001B[A\n",
      " 35%|██████████████▋                           | 40/114 [00:29<00:56,  1.32it/s]\u001B[A\n",
      " 36%|███████████████                           | 41/114 [00:30<00:54,  1.35it/s]\u001B[A\n",
      " 37%|███████████████▍                          | 42/114 [00:30<00:54,  1.32it/s]\u001B[A\n",
      " 38%|███████████████▊                          | 43/114 [00:31<00:52,  1.35it/s]\u001B[A\n",
      " 39%|████████████████▏                         | 44/114 [00:32<00:53,  1.32it/s]\u001B[A\n",
      " 39%|████████████████▌                         | 45/114 [00:33<00:53,  1.30it/s]\u001B[A\n",
      " 40%|████████████████▉                         | 46/114 [00:33<00:50,  1.33it/s]\u001B[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:34<00:49,  1.36it/s]\u001B[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:35<00:47,  1.38it/s]\u001B[A\n",
      " 43%|██████████████████                        | 49/114 [00:36<00:48,  1.34it/s]\u001B[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:36<00:47,  1.36it/s]\u001B[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:37<00:49,  1.28it/s]\u001B[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:38<00:47,  1.31it/s]\u001B[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:39<00:48,  1.25it/s]\u001B[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:40<00:46,  1.30it/s]\u001B[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:40<00:45,  1.28it/s]\u001B[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:41<00:43,  1.32it/s]\u001B[A\n",
      " 50%|█████████████████████                     | 57/114 [00:42<00:43,  1.30it/s]\u001B[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:43<00:41,  1.34it/s]\u001B[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:43<00:40,  1.35it/s]\u001B[A\n",
      " 53%|██████████████████████                    | 60/114 [00:44<00:40,  1.33it/s]\u001B[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:45<00:38,  1.36it/s]\u001B[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:46<00:39,  1.32it/s]\u001B[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:46<00:37,  1.35it/s]\u001B[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:47<00:38,  1.31it/s]\u001B[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:48<00:37,  1.30it/s]\u001B[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:48<00:34,  1.39it/s]\u001B[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:49<00:34,  1.35it/s]\u001B[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:50<00:33,  1.37it/s]\u001B[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:51<00:32,  1.39it/s]\u001B[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:51<00:31,  1.39it/s]\u001B[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:52<00:30,  1.41it/s]\u001B[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:53<00:30,  1.36it/s]\u001B[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:54<00:29,  1.38it/s]\u001B[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:54<00:28,  1.39it/s]\u001B[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:55<00:27,  1.40it/s]\u001B[A\n",
      " 67%|████████████████████████████              | 76/114 [00:56<00:26,  1.41it/s]\u001B[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:56<00:26,  1.40it/s]\u001B[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:57<00:25,  1.42it/s]\u001B[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:58<00:24,  1.43it/s]\u001B[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:59<00:24,  1.37it/s]\u001B[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:59<00:24,  1.33it/s]\u001B[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [01:00<00:24,  1.30it/s]\u001B[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [01:01<00:23,  1.34it/s]\u001B[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [01:02<00:23,  1.26it/s]\u001B[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [01:02<00:21,  1.36it/s]\u001B[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [01:03<00:21,  1.33it/s]\u001B[A\n",
      " 76%|████████████████████████████████          | 87/114 [01:04<00:21,  1.25it/s]\u001B[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [01:05<00:21,  1.21it/s]\u001B[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [01:06<00:19,  1.27it/s]\u001B[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [01:06<00:19,  1.26it/s]\u001B[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [01:07<00:17,  1.31it/s]\u001B[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [01:08<00:17,  1.29it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [01:09<00:16,  1.28it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [01:09<00:15,  1.32it/s]\u001B[A\n",
      " 83%|███████████████████████████████████       | 95/114 [01:10<00:14,  1.34it/s]\u001B[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [01:11<00:13,  1.37it/s]\u001B[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [01:11<00:12,  1.39it/s]\u001B[A\n",
      " 86%|████████████████████████████████████      | 98/114 [01:12<00:11,  1.40it/s]\u001B[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [01:13<00:10,  1.46it/s]\u001B[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [01:14<00:09,  1.46it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [01:14<00:09,  1.44it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [01:15<00:08,  1.45it/s]\u001B[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [01:16<00:08,  1.33it/s]\u001B[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [01:17<00:07,  1.30it/s]\u001B[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [01:17<00:06,  1.29it/s]\u001B[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [01:18<00:06,  1.28it/s]\u001B[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:19<00:05,  1.32it/s]\u001B[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:20<00:04,  1.35it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:20<00:03,  1.37it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:21<00:02,  1.39it/s]\u001B[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:22<00:02,  1.24it/s]\u001B[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:23<00:01,  1.25it/s]\u001B[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:24<00:00,  1.07it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 0.2843213975429535, 'eval_runtime': 85.8775, 'eval_samples_per_second': 21.088, 'eval_steps_per_second': 1.327, 'epoch': 0.59}\n",
      " 15%|█████▌                                | 150/1016 [28:32<2:21:50,  9.83s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:25<00:00,  1.13it/s]\u001B[A\n",
      "{'loss': 0.2814, 'learning_rate': 7.259173205916098e-05, 'epoch': 0.79}         \u001B[A\n",
      " 20%|███████▍                              | 200/1016 [36:34<2:10:31,  9.60s/it][INFO|trainer.py:3165] 2023-09-01 06:44:13,353 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3167] 2023-09-01 06:44:13,353 >>   Num examples = 1811\n",
      "[INFO|trainer.py:3170] 2023-09-01 06:44:13,353 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:44,  2.50it/s]\u001B[A\n",
      "  3%|█▏                                         | 3/114 [00:01<01:03,  1.76it/s]\u001B[A\n",
      "  4%|█▌                                         | 4/114 [00:02<01:07,  1.62it/s]\u001B[A\n",
      "  4%|█▉                                         | 5/114 [00:03<01:14,  1.47it/s]\u001B[A\n",
      "  5%|██▎                                        | 6/114 [00:03<01:14,  1.46it/s]\u001B[A\n",
      "  6%|██▋                                        | 7/114 [00:04<01:13,  1.45it/s]\u001B[A\n",
      "  7%|███                                        | 8/114 [00:05<01:16,  1.38it/s]\u001B[A\n",
      "  8%|███▍                                       | 9/114 [00:06<01:15,  1.39it/s]\u001B[A\n",
      "  9%|███▋                                      | 10/114 [00:06<01:17,  1.35it/s]\u001B[A\n",
      " 10%|████                                      | 11/114 [00:07<01:13,  1.41it/s]\u001B[A\n",
      " 11%|████▍                                     | 12/114 [00:08<01:14,  1.38it/s]\u001B[A\n",
      " 11%|████▊                                     | 13/114 [00:09<01:15,  1.34it/s]\u001B[A\n",
      " 12%|█████▏                                    | 14/114 [00:09<01:13,  1.36it/s]\u001B[A\n",
      " 13%|█████▌                                    | 15/114 [00:10<01:11,  1.38it/s]\u001B[A\n",
      " 14%|█████▉                                    | 16/114 [00:11<01:13,  1.34it/s]\u001B[A\n",
      " 15%|██████▎                                   | 17/114 [00:12<01:17,  1.25it/s]\u001B[A\n",
      " 16%|██████▋                                   | 18/114 [00:12<01:16,  1.26it/s]\u001B[A\n",
      " 17%|███████                                   | 19/114 [00:13<01:15,  1.26it/s]\u001B[A\n",
      " 18%|███████▎                                  | 20/114 [00:14<01:12,  1.30it/s]\u001B[A\n",
      " 18%|███████▋                                  | 21/114 [00:15<01:12,  1.28it/s]\u001B[A\n",
      " 19%|████████                                  | 22/114 [00:16<01:11,  1.28it/s]\u001B[A\n",
      " 20%|████████▍                                 | 23/114 [00:16<01:08,  1.32it/s]\u001B[A\n",
      " 21%|████████▊                                 | 24/114 [00:17<01:06,  1.35it/s]\u001B[A\n",
      " 22%|█████████▏                                | 25/114 [00:18<01:02,  1.42it/s]\u001B[A\n",
      " 23%|█████████▌                                | 26/114 [00:18<01:04,  1.36it/s]\u001B[A\n",
      " 24%|█████████▉                                | 27/114 [00:19<01:05,  1.34it/s]\u001B[A\n",
      " 25%|██████████▎                               | 28/114 [00:20<01:08,  1.26it/s]\u001B[A\n",
      " 25%|██████████▋                               | 29/114 [00:21<01:05,  1.31it/s]\u001B[A\n",
      " 26%|███████████                               | 30/114 [00:21<01:02,  1.34it/s]\u001B[A\n",
      " 27%|███████████▍                              | 31/114 [00:22<01:01,  1.35it/s]\u001B[A\n",
      " 28%|███████████▊                              | 32/114 [00:23<00:58,  1.39it/s]\u001B[A\n",
      " 29%|████████████▏                             | 33/114 [00:24<01:00,  1.35it/s]\u001B[A\n",
      " 30%|████████████▌                             | 34/114 [00:24<00:58,  1.37it/s]\u001B[A\n",
      " 31%|████████████▉                             | 35/114 [00:25<00:54,  1.44it/s]\u001B[A\n",
      " 32%|█████████████▎                            | 36/114 [00:26<00:56,  1.38it/s]\u001B[A\n",
      " 32%|█████████████▋                            | 37/114 [00:26<00:53,  1.45it/s]\u001B[A\n",
      " 33%|██████████████                            | 38/114 [00:27<00:56,  1.34it/s]\u001B[A\n",
      " 34%|██████████████▎                           | 39/114 [00:28<00:57,  1.31it/s]\u001B[A\n",
      " 35%|██████████████▋                           | 40/114 [00:29<00:55,  1.34it/s]\u001B[A\n",
      " 36%|███████████████                           | 41/114 [00:29<00:53,  1.37it/s]\u001B[A\n",
      " 37%|███████████████▍                          | 42/114 [00:30<00:52,  1.38it/s]\u001B[A\n",
      " 38%|███████████████▊                          | 43/114 [00:31<00:52,  1.34it/s]\u001B[A\n",
      " 39%|████████████████▏                         | 44/114 [00:32<00:51,  1.37it/s]\u001B[A\n",
      " 39%|████████████████▌                         | 45/114 [00:32<00:51,  1.33it/s]\u001B[A\n",
      " 40%|████████████████▉                         | 46/114 [00:33<00:50,  1.36it/s]\u001B[A\n",
      " 41%|█████████████████▎                        | 47/114 [00:34<00:47,  1.42it/s]\u001B[A\n",
      " 42%|█████████████████▋                        | 48/114 [00:34<00:47,  1.38it/s]\u001B[A\n",
      " 43%|██████████████████                        | 49/114 [00:35<00:44,  1.45it/s]\u001B[A\n",
      " 44%|██████████████████▍                       | 50/114 [00:36<00:48,  1.33it/s]\u001B[A\n",
      " 45%|██████████████████▊                       | 51/114 [00:37<00:46,  1.35it/s]\u001B[A\n",
      " 46%|███████████████████▏                      | 52/114 [00:38<00:46,  1.33it/s]\u001B[A\n",
      " 46%|███████████████████▌                      | 53/114 [00:38<00:48,  1.25it/s]\u001B[A\n",
      " 47%|███████████████████▉                      | 54/114 [00:39<00:46,  1.30it/s]\u001B[A\n",
      " 48%|████████████████████▎                     | 55/114 [00:40<00:45,  1.29it/s]\u001B[A\n",
      " 49%|████████████████████▋                     | 56/114 [00:41<00:43,  1.33it/s]\u001B[A\n",
      " 50%|█████████████████████                     | 57/114 [00:41<00:43,  1.30it/s]\u001B[A\n",
      " 51%|█████████████████████▎                    | 58/114 [00:42<00:41,  1.34it/s]\u001B[A\n",
      " 52%|█████████████████████▋                    | 59/114 [00:43<00:40,  1.36it/s]\u001B[A\n",
      " 53%|██████████████████████                    | 60/114 [00:44<00:42,  1.28it/s]\u001B[A\n",
      " 54%|██████████████████████▍                   | 61/114 [00:44<00:40,  1.31it/s]\u001B[A\n",
      " 54%|██████████████████████▊                   | 62/114 [00:45<00:40,  1.30it/s]\u001B[A\n",
      " 55%|███████████████████████▏                  | 63/114 [00:46<00:38,  1.34it/s]\u001B[A\n",
      " 56%|███████████████████████▌                  | 64/114 [00:47<00:38,  1.31it/s]\u001B[A\n",
      " 57%|███████████████████████▉                  | 65/114 [00:47<00:36,  1.34it/s]\u001B[A\n",
      " 58%|████████████████████████▎                 | 66/114 [00:48<00:36,  1.31it/s]\u001B[A\n",
      " 59%|████████████████████████▋                 | 67/114 [00:49<00:34,  1.35it/s]\u001B[A\n",
      " 60%|█████████████████████████                 | 68/114 [00:50<00:33,  1.37it/s]\u001B[A\n",
      " 61%|█████████████████████████▍                | 69/114 [00:50<00:33,  1.33it/s]\u001B[A\n",
      " 61%|█████████████████████████▊                | 70/114 [00:51<00:32,  1.36it/s]\u001B[A\n",
      " 62%|██████████████████████████▏               | 71/114 [00:52<00:31,  1.36it/s]\u001B[A\n",
      " 63%|██████████████████████████▌               | 72/114 [00:53<00:31,  1.34it/s]\u001B[A\n",
      " 64%|██████████████████████████▉               | 73/114 [00:53<00:29,  1.37it/s]\u001B[A\n",
      " 65%|███████████████████████████▎              | 74/114 [00:54<00:29,  1.37it/s]\u001B[A\n",
      " 66%|███████████████████████████▋              | 75/114 [00:55<00:27,  1.40it/s]\u001B[A\n",
      " 67%|████████████████████████████              | 76/114 [00:56<00:28,  1.35it/s]\u001B[A\n",
      " 68%|████████████████████████████▎             | 77/114 [00:56<00:26,  1.38it/s]\u001B[A\n",
      " 68%|████████████████████████████▋             | 78/114 [00:57<00:25,  1.43it/s]\u001B[A\n",
      " 69%|█████████████████████████████             | 79/114 [00:58<00:25,  1.39it/s]\u001B[A\n",
      " 70%|█████████████████████████████▍            | 80/114 [00:58<00:24,  1.40it/s]\u001B[A\n",
      " 71%|█████████████████████████████▊            | 81/114 [00:59<00:24,  1.35it/s]\u001B[A\n",
      " 72%|██████████████████████████████▏           | 82/114 [01:00<00:23,  1.37it/s]\u001B[A\n",
      " 73%|██████████████████████████████▌           | 83/114 [01:00<00:21,  1.43it/s]\u001B[A\n",
      " 74%|██████████████████████████████▉           | 84/114 [01:01<00:21,  1.39it/s]\u001B[A\n",
      " 75%|███████████████████████████████▎          | 85/114 [01:02<00:21,  1.38it/s]\u001B[A\n",
      " 75%|███████████████████████████████▋          | 86/114 [01:03<00:21,  1.31it/s]\u001B[A\n",
      " 76%|████████████████████████████████          | 87/114 [01:04<00:20,  1.29it/s]\u001B[A\n",
      " 77%|████████████████████████████████▍         | 88/114 [01:04<00:21,  1.23it/s]\u001B[A\n",
      " 78%|████████████████████████████████▊         | 89/114 [01:05<00:19,  1.27it/s]\u001B[A\n",
      " 79%|█████████████████████████████████▏        | 90/114 [01:06<00:18,  1.33it/s]\u001B[A\n",
      " 80%|█████████████████████████████████▌        | 91/114 [01:07<00:17,  1.30it/s]\u001B[A\n",
      " 81%|█████████████████████████████████▉        | 92/114 [01:07<00:17,  1.29it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▎       | 93/114 [01:08<00:16,  1.28it/s]\u001B[A\n",
      " 82%|██████████████████████████████████▋       | 94/114 [01:09<00:14,  1.36it/s]\u001B[A\n",
      " 83%|███████████████████████████████████       | 95/114 [01:10<00:13,  1.39it/s]\u001B[A\n",
      " 84%|███████████████████████████████████▎      | 96/114 [01:10<00:13,  1.35it/s]\u001B[A\n",
      " 85%|███████████████████████████████████▋      | 97/114 [01:11<00:11,  1.49it/s]\u001B[A\n",
      " 86%|████████████████████████████████████      | 98/114 [01:12<00:10,  1.54it/s]\u001B[A\n",
      " 87%|████████████████████████████████████▍     | 99/114 [01:12<00:09,  1.50it/s]\u001B[A\n",
      " 88%|███████████████████████████████████▉     | 100/114 [01:13<00:09,  1.48it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▎    | 101/114 [01:14<00:08,  1.53it/s]\u001B[A\n",
      " 89%|████████████████████████████████████▋    | 102/114 [01:14<00:08,  1.44it/s]\u001B[A\n",
      " 90%|█████████████████████████████████████    | 103/114 [01:15<00:08,  1.37it/s]\u001B[A\n",
      " 91%|█████████████████████████████████████▍   | 104/114 [01:16<00:07,  1.33it/s]\u001B[A\n",
      " 92%|█████████████████████████████████████▊   | 105/114 [01:17<00:06,  1.31it/s]\u001B[A\n",
      " 93%|██████████████████████████████████████   | 106/114 [01:18<00:06,  1.29it/s]\u001B[A\n",
      " 94%|██████████████████████████████████████▍  | 107/114 [01:18<00:05,  1.33it/s]\u001B[A\n",
      " 95%|██████████████████████████████████████▊  | 108/114 [01:19<00:04,  1.30it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▏ | 109/114 [01:20<00:03,  1.34it/s]\u001B[A\n",
      " 96%|███████████████████████████████████████▌ | 110/114 [01:20<00:02,  1.37it/s]\u001B[A\n",
      " 97%|███████████████████████████████████████▉ | 111/114 [01:21<00:02,  1.28it/s]\u001B[A\n",
      " 98%|████████████████████████████████████████▎| 112/114 [01:22<00:01,  1.27it/s]\u001B[A\n",
      " 99%|████████████████████████████████████████▋| 113/114 [01:23<00:00,  1.08it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 0.2689819931983948, 'eval_runtime': 85.1717, 'eval_samples_per_second': 21.263, 'eval_steps_per_second': 1.338, 'epoch': 0.79}\n",
      " 20%|███████▍                              | 200/1016 [37:59<2:10:31,  9.60s/it]\n",
      "100%|█████████████████████████████████████████| 114/114 [01:24<00:00,  1.14it/s]\u001B[A\n",
      "                                                                                \u001B[A09/01/2023 06:45:38 - INFO - llmtuner.tuner.core.trainer - Saving model checkpoint to ../checkpoints/sa_8_31/checkpoint-200\n",
      "{'loss': 0.2581, 'learning_rate': 6.86319322055637e-05, 'epoch': 0.98}          \n",
      " 25%|█████████▎                            | 250/1016 [46:03<2:01:55,  9.55s/it][INFO|trainer.py:3165] 2023-09-01 06:53:42,438 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3167] 2023-09-01 06:53:42,438 >>   Num examples = 1811\n",
      "[INFO|trainer.py:3170] 2023-09-01 06:53:42,438 >>   Batch size = 16\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▊                                          | 2/114 [00:00<00:33,  3.31it/s]\u001B[A\n",
      "  3%|█▏                                         | 3/114 [00:01<01:00,  1.82it/s]\u001B[A\n",
      "  4%|█▌                                         | 4/114 [00:02<01:07,  1.62it/s]\u001B[A\n",
      "  4%|█▉                                         | 5/114 [00:02<01:09,  1.57it/s]\u001B[A\n",
      "  5%|██▎                                        | 6/114 [00:03<01:14,  1.45it/s]\u001B[A\n",
      "  6%|██▋                                        | 7/114 [00:04<01:10,  1.51it/s]\u001B[A\n",
      "  7%|███                                        | 8/114 [00:05<01:19,  1.34it/s]\u001B[A\n",
      "  8%|███▍                                       | 9/114 [00:05<01:18,  1.34it/s]\u001B[A\n",
      "  9%|███▋                                      | 10/114 [00:07<01:26,  1.20it/s]\u001B[A\n",
      " 10%|████                                      | 11/114 [00:07<01:18,  1.31it/s]\u001B[A\n",
      " 11%|████▍                                     | 12/114 [00:08<01:24,  1.21it/s]\u001B[A\n",
      " 11%|████▊                                     | 13/114 [00:09<01:26,  1.17it/s]\u001B[A\n",
      " 12%|█████▏                                    | 14/114 [00:10<01:31,  1.09it/s]\u001B[A\n",
      " 13%|█████▌                                    | 15/114 [00:11<01:27,  1.13it/s]\u001B[A\n",
      " 14%|█████▉                                    | 16/114 [00:12<01:33,  1.05it/s]\u001B[A\n",
      " 15%|██████▎                                   | 17/114 [00:13<01:37,  1.00s/it]\u001B[A\n",
      " 16%|██████▋                                   | 18/114 [00:14<01:36,  1.00s/it]\u001B[A\n",
      " 17%|███████                                   | 19/114 [00:15<01:34,  1.00it/s]\u001B[A\n",
      " 18%|███████▎                                  | 20/114 [00:16<01:29,  1.05it/s]\u001B[A\n",
      " 18%|███████▋                                  | 21/114 [00:17<01:31,  1.02it/s]\u001B[A\n",
      " 19%|████████                                  | 22/114 [00:18<01:31,  1.00it/s]\u001B[A"
     ]
    }
   ],
   "source": [
    "!python ./train_bash.py \\\n",
    "    --do_train \\\n",
    "    --seed 3825 \\\n",
    "    --model_name_or_path /hy-tmp/llama-2-7b-hf \\\n",
    "    --template llama2 \\\n",
    "    --dataset sa_train \\\n",
    "    --dataset_dir ../data \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_rank 64 \\\n",
    "    --output_dir ../checkpoints/sa_9_1 \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --logging_steps 50 \\\n",
    "    --save_steps 100 \\\n",
    "    --val_size 0.1 \\\n",
    "    --learning_rate 8e-5 \\\n",
    "    --resume_lora_training False \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --load_best_model_at_end \\\n",
    "    --fp16 \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_find_unused_parameters False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'domain': 'Books', 'instruction': 'In the ACOS task, you are required to extract the Aspect (a specific detail of the topic), Category (the general domain of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (the descriptive sentiment regarding the aspect). For the provided sentence from the Books domain, extract ACOS quadruples in the format: \"[Aspect, Category, Sentiment, Opinion],...\". If any element is missing, replace it with \"NULL\". Sentence: \\n\"Unfortunately , it was n\\'t up to par with her past works .\"\\n            ', 'input': '', 'output': '[NULL, Book#General, NEG, Unfortunately]', 'task': 'acos'}, {'id': 2, 'domain': 'Books', 'instruction': 'Engage in the ACOS activity. Here, you\\'ll distinguish the Aspect (an individual characteristic), Category (which domain this aspect falls under), Sentiment (either Postive, Negative, or Neutral), and Opinion (the elaboration of the sentiment). With a sentence from the Books, represent ACOS in this manner: \"[Aspect, Category, Sentiment, Opinion],...\". \"NULL\" should be used for elements that are not identified. Dive into the sentence: \\n\"Not only was the whole plane crash into a very specific location bad , but then bones hidden for 30 years suddenly get found at the exact right time .\"\\n            ', 'input': '', 'output': '[NULL, Content#Plot, NEG, bad]', 'task': 'acos'}, {'id': 3, 'domain': 'Books', 'instruction': 'You\\'re tasked with the ACOS extraction. This involves recognizing the Aspect (particular element of interest), Category (overall domain of the aspect), Sentiment (can be Postive, Negative, or Neutral), and the Opinion (how the sentiment is described). Given a sentence from the Books, list the ACOS details as: \"[Aspect, Category, Sentiment, Opinion],...\". If a component isn\\'t present, write \"NULL\". Consider this sentence: \\n\"A very good read from the author who saw it all , up close and very personal .\"\\n            ', 'input': '', 'output': '[read, Book#General, POS, very good]', 'task': 'acos'}, {'id': 4, 'domain': 'Books', 'instruction': 'Here\\'s a sentence from Books for analysis: \\n\"This book was worth the wait , and I will keep watching for the next novel ... no matter how long it takes !\"\\nFor ASTE, pinpoint the Aspect (detail expressing sentiment), the Sentiment (Positive, Negative, or Neutral), and describe the sentiment using the Opinion. Represent your extraction as: \"[Aspect, Sentiment, Opinion],...\". Use \"NULL\" for indeterminate sections.\\n', 'input': '', 'output': '[book, POS, worth],[next novel, POS, NULL]', 'task': 'aste'}, {'id': 5, 'domain': 'Books', 'instruction': 'Here\\'s a sentence from Books for analysis: \\n\"An excellent page turner .\"\\nFor ASTE, pinpoint the Aspect (detail expressing sentiment), the Sentiment (Positive, Negative, or Neutral), and describe the sentiment using the Opinion. Represent your extraction as: \"[Aspect, Sentiment, Opinion],...\". Use \"NULL\" for indeterminate sections.\\n', 'input': '', 'output': '[page turner, POS, excellent]', 'task': 'aste'}, {'id': 6, 'domain': 'Books', 'instruction': 'You\\'re tasked with the ACOS extraction. This involves recognizing the Aspect (particular element of interest), Category (overall domain of the aspect), Sentiment (can be Postive, Negative, or Neutral), and the Opinion (how the sentiment is described). Given a sentence from the Books, list the ACOS details as: \"[Aspect, Category, Sentiment, Opinion],...\". If a component isn\\'t present, write \"NULL\". Consider this sentence: \\n\"Jodie \\'s tantrums range from cursing and kicking to multiple personalities , the level of abuse she experienced is beyond what most people can imagine.This book can be hard to read at times but Cathy \\'s strength and her dedication to helping this child is impressive , it held a lot of good ideas and lessons about taking care of children in pain .\"\\n            ', 'input': '', 'output': '[NULL, Content#Plot, POS, good],[Cathy, Content#Characters, POS, impressive],[book, Content#Plot, NEG, hard]', 'task': 'acos'}, {'id': 7, 'domain': 'Books', 'instruction': 'You\\'re tasked with the ACOS extraction. This involves recognizing the Aspect (particular element of interest), Category (overall domain of the aspect), Sentiment (can be Postive, Negative, or Neutral), and the Opinion (how the sentiment is described). Given a sentence from the Books, list the ACOS details as: \"[Aspect, Category, Sentiment, Opinion],...\". If a component isn\\'t present, write \"NULL\". Consider this sentence: \\n\"( this was really at odds with her personality in the book ) . Confusing and just plain awful to read .\"\\n            ', 'input': '', 'output': '[NULL, Content#Plot, NEG, just plain awful],[NULL, Content#Plot, NEG, Confusing]', 'task': 'acos'}, {'id': 8, 'domain': 'Books', 'instruction': 'For this task, pinpoint the following from the sentence: Aspect (unique feature or detail), Category (major domain pertaining to the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (expressed sentiment about the aspect). Given a sentence from Books, structure your ACOS findings as: \"[Aspect, Category, Sentiment, Opinion],...\". In the absence of any detail, fill with \"NULL\". Here\\'s your sentence: \\n\"love these two characters , and have been following them all along but am nearing the ends of books , need more\"\\n            ', 'input': '', 'output': '[characters, Content#Characters, POS, NULL],[characters, Content#Characters, POS, love],[NULL, Book#General, POS, NULL]', 'task': 'acos'}, {'id': 9, 'domain': 'Books', 'instruction': 'In the ACOS task, you are required to extract the Aspect (a specific detail of the topic), Category (the general domain of the aspect), Sentiment (Postive, Negative, Neutral), and Opinion (the descriptive sentiment regarding the aspect). For the provided sentence from the Books domain, extract ACOS quadruples in the format: \"[Aspect, Category, Sentiment, Opinion],...\". If any element is missing, replace it with \"NULL\". Sentence: \\n\"There are also a couple of good storylines involving some of Patrick \\'s co-workers.I don\\'t know if it was the story or just that this is the fourth book I have read by this author and in this series , but I was able to figure out the twist really soon .\"\\n            ', 'input': '', 'output': '[storylines, Content#Plot, POS, good]', 'task': 'acos'}, {'id': 10, 'domain': 'Books', 'instruction': 'Here\\'s a sentence from Books for analysis: \\n\"Thanks for the great transaction !\"\\nFor ASTE, pinpoint the Aspect (detail expressing sentiment), the Sentiment (Positive, Negative, or Neutral), and describe the sentiment using the Opinion. Represent your extraction as: \"[Aspect, Sentiment, Opinion],...\". Use \"NULL\" for indeterminate sections.\\n', 'input': '', 'output': '[transaction, POS, Thanks]', 'task': 'aste'}]\n",
      "18104\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 要加载，不能每次都生成，要不然会有重复\n",
    "test_path = \"../data/inst/test.json\"\n",
    "\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(test_data[:10])\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c494ece527945fdb71542dc8dcdcb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/05/2023 05:50:40 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA\n",
      "09/05/2023 05:51:04 - INFO - llmtuner.tuner.core.adapter - Merged 1 model checkpoint(s).\n",
      "09/05/2023 05:51:04 - INFO - llmtuner.tuner.core.adapter - Loaded fine-tuned model from checkpoint(s): ../checkpoints/sa_9_1\n",
      "09/05/2023 05:51:12 - INFO - llmtuner.tuner.core.loader - trainable params: 0 || all params: 6738415616 || trainable%: 0.0000\n",
      "09/05/2023 05:51:15 - INFO - llmtuner.extras.template - Add pad token: <unk>\n"
     ]
    }
   ],
   "source": [
    "from llmtuner import ChatModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "args = {\n",
    "    \"model_name_or_path\": \"/hy-tmp/llama-2-7b-hf\",\n",
    "    \"checkpoint_dir\": \"../checkpoints/sa_9_1\",\n",
    "    \"template\": \"llama2\",\n",
    "    \"finetuning_type\": \"lora\",\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.7,\n",
    "    \"repetition_penalty\": 1.2\n",
    "}\n",
    "\n",
    "chat_model = ChatModel(args=args)\n",
    "\n",
    "def generate(query, history = []):\n",
    "    res, _ = chat_model.chat(query, history)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "import re\n",
    "\n",
    "def parse_output(s):\n",
    "    groups = re.findall(r'\\[(.*?)\\]', s)\n",
    "    elements = [re.split(r',\\s*', group) for group in groups]\n",
    "    return elements\n",
    "\n",
    "def get_metric(y_trues, y_preds, id=None):\n",
    "    y_true_binarys, y_pred_binarys = [], []\n",
    "    for y_true, y_pred in zip(y_trues, y_preds):\n",
    "        # 先把所有出现的元组进行合并\n",
    "        # print(y_true[0][0])\n",
    "        if id != None:\n",
    "            y_true_set = set(item[id] for item in y_true)\n",
    "            y_pred_set = set(item[id] for item in y_pred)\n",
    "        else:\n",
    "            y_true_set = set(str(item) for item in y_true)\n",
    "            y_pred_set = set(str(item) for item in y_pred)\n",
    "        all_tuples = y_true_set.union(y_pred_set)\n",
    "        # print(all_tuples)\n",
    "\n",
    "        # 转换为 0-1 表示形式\n",
    "        y_true_binary = [1 if item in y_true_set else 0 for item in all_tuples]\n",
    "        y_pred_binary = [1 if item in y_pred_set else 0 for item in all_tuples]\n",
    "\n",
    "        y_true_binarys.extend(y_true_binary)\n",
    "        y_pred_binarys.extend(y_pred_binary)\n",
    "\n",
    "    # print(y_true_binarys, y_pred_binarys)\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(y_true_binarys, y_pred_binarys)\n",
    "    precision = precision_score(y_true_binarys, y_pred_binarys)\n",
    "    recall = recall_score(y_true_binarys, y_pred_binarys)\n",
    "    f1 = f1_score(y_true_binarys, y_pred_binarys)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def get_metrics(y_trues, y_preds, task, item='all'):\n",
    "    acc, p, r, f1 = get_metric(y_trues, y_preds)\n",
    "    print(f\"{task} Accuracy: {acc:.4f} Precision: {p:.4f} Recall: {r:.4f} F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # items_dict = {0: 'Aspect', 1: 'Category', 2: 'Sentiment', 3: 'Opinion'}\n",
    "    # for k, v in items_dict.items():\n",
    "    #     acc, p, r, f1 = get_metric(y_trues, y_preds, id=k)\n",
    "    #     print(f\"{v} Accuracy: {acc:.4f} Precision: {p:.4f} Recall: {r:.4f} F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 示例数据\n",
    "# y_true = [\n",
    "#     [['book', 'Book#General', 'POS', 'entertaining']],\n",
    "#     [['book', 'Book#Quality', 'POS', 'Thoroughly'], ['book', 'Book#Quality', 'POS', 'easily']],\n",
    "#     [['book', 'Book#General', 'NEG', 'NULL']]\n",
    "# ]\n",
    "\n",
    "# y_pred = [\n",
    "#     [['book', 'Book#General', 'POS', 'entertaining'], ['book', 'Book#General', 'POS', 'amusing']],\n",
    "#     [['book', 'Book#Quality', 'NEG', 'Thoroughly'], ['book', 'Book#General', 'POS', 'easily']],\n",
    "#     [['book', 'Book#General', 'POS', 'not good']]\n",
    "# ]\n",
    "\n",
    "# get_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 4770/18104 [22:40<52:08,  4.26it/s]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tasks = ['acos', 'aste', 'as', 'ate', 'ote']\n",
    "\n",
    "for task in tasks:\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    error_records = []  # 用于保存错误的记录\n",
    "    for data in tqdm(test_data):\n",
    "        if data['task'] != task:\n",
    "            continue\n",
    "\n",
    "        instruction = data['instruction']\n",
    "        true_output = parse_output(data['output'])\n",
    "        \n",
    "        try:\n",
    "            predicted_output_str = generate(instruction)\n",
    "            predicted_output = parse_output(predicted_output_str)\n",
    "        except Exception as e:\n",
    "            # 如果在生成或解析预测过程中出现异常，使用一个空表作为预测输出\n",
    "            predicted_output = []\n",
    "            predicted_output_str = str(e)  # 在这里，我们将异常保存为预测的输出，这样我们可以在CSV中看到它\n",
    "\n",
    "        # 如果预测的输出与真实的输出不匹配，将它们添加到错误记录中\n",
    "        if predicted_output != true_output:\n",
    "            error_records.append({\n",
    "                'task': data['task'],\n",
    "                'instruction': instruction,\n",
    "                'True Output': true_output,\n",
    "                'Predicted Output String': predicted_output\n",
    "            })\n",
    "\n",
    "        y_trues.append(true_output)\n",
    "        y_preds.append(predicted_output)\n",
    "\n",
    "        # print(y_trues, y_preds)\n",
    "\n",
    "    # 保存错误记录到CSV\n",
    "    if error_records:\n",
    "        df_errors = pd.DataFrame(error_records)\n",
    "        df_errors.to_csv(f'error_records.tsv', index=False, sep='\\t')\n",
    "\n",
    "    get_metrics(y_trues, y_preds, task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|██████████| 100/100 [02:16<00:00,  1.37s/it]\n",
    "Overall Accuracy: 0.1542 Precision: 0.2727 Recall: 0.2617 F1 Score: 0.2671\n",
    "Aspect Accuracy: 0.3977 Precision: 0.6182 Recall: 0.5271 F1 Score: 0.5690\n",
    "Category Accuracy: 0.4870 Precision: 0.7009 Recall: 0.6148 F1 Score: 0.6550\n",
    "Sentiment Accuracy: 0.7600 Precision: 0.8879 Recall: 0.8407 F1 Score: 0.8636\n",
    "Opinion Accuracy: 0.5027 Precision: 0.6763 Recall: 0.6620 F1 Score: 0.6690\n",
    "\n",
    "[8-31]\n",
    "100%|██████████| 3583/3583 [1:16:35<00:00,  1.28s/it]\n",
    "Overall Accuracy: 0.2818 Precision: 0.4452 Recall: 0.4342 F1 Score: 0.4396\n",
    "Aspect Accuracy: 0.5955 Precision: 0.7721 Recall: 0.7225 F1 Score: 0.7465\n",
    "Category Accuracy: 0.5350 Precision: 0.7170 Recall: 0.6783 F1 Score: 0.6971\n",
    "Sentiment Accuracy: 0.8746 Precision: 0.9427 Recall: 0.9237 F1 Score: 0.9331\n",
    "Opinion Accuracy: 0.6177 Precision: 0.7714 Recall: 0.7560 F1 Score: 0.7636\n",
    "\n",
    "100%|██████████| 3583/3583 [26:33<00:00,  2.25it/s] \n",
    "acos Accuracy: 0.3062 Precision: 0.4794 Recall: 0.4587 F1 Score: 0.4688\n",
    "100%|██████████| 3583/3583 [14:06<00:00,  4.23it/s]\n",
    "aste Accuracy: 0.2050 Precision: 0.3633 Recall: 0.3200 F1 Score: 0.3403\n",
    "100%|██████████| 3583/3583 [14:17<00:00,  4.18it/s]\n",
    "as Accuracy: 0.2116 Precision: 0.3477 Recall: 0.3510 F1 Score: 0.3493\n",
    "100%|██████████| 3583/3583 [07:04<00:00,  8.43it/s]\n",
    "ate Accuracy: 0.6484 Precision: 0.8152 Recall: 0.7601 F1 Score: 0.7867\n",
    "100%|██████████| 3583/3583 [08:02<00:00,  7.43it/s]\n",
    "ote Accuracy: 0.6498 Precision: 0.7852 Recall: 0.7902 F1 Score: 0.7877"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
